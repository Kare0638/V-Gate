import os
import time
from vllm import LLM, SamplingParams

class VGateEngine:
    def __init__(self, model_name="Qwen/Qwen2.5-1.5B-Instruct-AWQ"):
        print(f"Loading {model_name} with AWQ quantization for RTX 3060 optimization...")
        self.llm = LLM(
            model=model_name,
            quantization="awq",           # æ ¸å¿ƒ: å¼€å¯é‡åŒ–
            gpu_memory_utilization=0.7,   # é¢„ç•™ 30% æ˜¾å­˜ç»™ KV Cache
            max_model_len=2048,           # é™åˆ¶ä¸Šä¸‹æ–‡
            enforce_eager=True,           # å…³é—­ CUDA Graph ä»¥èŠ‚çœæ˜¾å­˜
            trust_remote_code=True
        )

    def chat_completions(self, prompt, max_tokens=256):
        # æž„é€ é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=max_tokens)
        
        # ðŸŸ¢ 1. å¼€å§‹æ‰‹åŠ¨è®¡æ—¶ (Wall Clock Time)
        start_time = time.perf_counter()
        
        # æ‰§è¡ŒæŽ¨ç†
        outputs = self.llm.generate([prompt], sampling_params)
        
        # ðŸ”´ 2. ç»“æŸè®¡æ—¶
        end_time = time.perf_counter()
        
        output = outputs[0]
        generated_text = output.outputs[0].text
        num_tokens = len(output.outputs[0].token_ids)
        
        # ðŸŸ¡ 3. èŽ·å– Metrics (å¸¦ Fallback æœºåˆ¶)
        metrics = output.metrics
        
        if metrics:
            # å¦‚æžœ vLLM ç»™äº†å†…éƒ¨æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨ (æ›´å‡†)
            ttft = metrics.first_token_time - metrics.arrival_time
            total_time = metrics.finished_time - metrics.first_token_time
        else:
            # ðŸ›¡ï¸ Fallback: å¦‚æžœ metrics æ˜¯ Noneï¼Œä½¿ç”¨æ‰‹åŠ¨è®¡æ—¶
            print("âš ï¸ Warning: vLLM internal metrics missing. Using wall-clock time.")
            ttft = 0.0  # ç¦»çº¿æ¨¡å¼ä¸‹å¾ˆéš¾æµ‹å‡† TTFTï¼Œæš‚ç½®ä¸º 0
            total_time = end_time - start_time

        # è®¡ç®— TPOT (é¿å…é™¤ä»¥é›¶)
        tpot = (total_time / num_tokens) if num_tokens > 0 else 0
        
        return {
            "text": generated_text,
            "ttft": ttft,
            "tpot": tpot,
            "total_tokens": num_tokens
        }

    def embeddings(self, input_text: str):
        """
        Placeholder for embeddings generation.
        In a real scenario, a dedicated embedding model would be loaded and used here.
        Returns a mock embedding for MVP.
        """
        print(f"VGateEngine: Generating mock embeddings for input: '{input_text}'")
        # Return a fixed mock embedding for now
        return {
            "object": "list",
            "data": [
                {
                    "object": "embedding",
                    "embedding": [i * 0.01 for i in range(1536)], # A longer mock embedding
                    "index": 0,
                }
            ],
            "model": "mock-embedding-model",
            "usage": {"prompt_tokens": len(input_text), "total_tokens": len(input_text)},
        }