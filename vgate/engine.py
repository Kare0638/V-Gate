import time
from typing import Optional

from vllm import LLM, SamplingParams

from vgate.config import ModelConfig, get_config


class VGateEngine:
    def __init__(self, model_config: Optional[ModelConfig] = None):
        """
        Initialize the vLLM engine with configuration.

        Args:
            model_config: Model configuration. If None, uses global config.
        """
        if model_config is None:
            model_config = get_config().model

        print(f"Loading {model_config.model_id} with {model_config.quantization} quantization...")
        self.llm = LLM(
            model=model_config.model_id,
            quantization=model_config.quantization,
            gpu_memory_utilization=model_config.gpu_memory_utilization,
            max_model_len=model_config.max_model_len,
            enforce_eager=model_config.enforce_eager,
            trust_remote_code=model_config.trust_remote_code,
        )

    def chat_completions(self, prompt, max_tokens=256):
        # æž„é€ é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=max_tokens)
        
        # ðŸŸ¢ 1. å¼€å§‹æ‰‹åŠ¨è®¡æ—¶ (Wall Clock Time)
        start_time = time.perf_counter()
        
        # æ‰§è¡ŒæŽ¨ç†
        outputs = self.llm.generate([prompt], sampling_params)
        
        # ðŸ”´ 2. ç»“æŸè®¡æ—¶
        end_time = time.perf_counter()
        
        output = outputs[0]
        generated_text = output.outputs[0].text
        num_tokens = len(output.outputs[0].token_ids)
        
        # ðŸŸ¡ 3. èŽ·å– Metrics (å¸¦ Fallback æœºåˆ¶)
        metrics = output.metrics
        
        if metrics:
            # å¦‚æžœ vLLM ç»™äº†å†…éƒ¨æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨ (æ›´å‡†)
            ttft = metrics.first_token_time - metrics.arrival_time
            total_time = metrics.finished_time - metrics.first_token_time
        else:
            # ðŸ›¡ï¸ Fallback: å¦‚æžœ metrics æ˜¯ Noneï¼Œä½¿ç”¨æ‰‹åŠ¨è®¡æ—¶
            print("âš ï¸ Warning: vLLM internal metrics missing. Using wall-clock time.")
            ttft = 0.0  # ç¦»çº¿æ¨¡å¼ä¸‹å¾ˆéš¾æµ‹å‡† TTFTï¼Œæš‚ç½®ä¸º 0
            total_time = end_time - start_time

        # è®¡ç®— TPOT (é¿å…é™¤ä»¥é›¶)
        tpot = (total_time / num_tokens) if num_tokens > 0 else 0
        
        return {
            "text": generated_text,
            "ttft": ttft,
            "tpot": tpot,
            "total_tokens": num_tokens
        }

    def embeddings(self, input_text: str):
        """
        Placeholder for embeddings generation.
        In a real scenario, a dedicated embedding model would be loaded and used here.
        Returns a mock embedding for MVP.
        """
        print(f"VGateEngine: Generating mock embeddings for input: '{input_text}'")
        # Return a fixed mock embedding for now
        return {
            "object": "list",
            "data": [
                {
                    "object": "embedding",
                    "embedding": [i * 0.01 for i in range(1536)], # A longer mock embedding
                    "index": 0,
                }
            ],
            "model": "mock-embedding-model",
            "usage": {"prompt_tokens": len(input_text), "total_tokens": len(input_text)},
        }